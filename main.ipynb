{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a668eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/08 21:45:23 WARN Utils: Your hostname, ra141 resolves to a loopback address: 127.0.1.1; using 192.168.1.115 instead (on interface enp0s31f6)\n",
      "21/08/08 21:45:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/08/08 21:45:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql\n",
    "from pyspark.sql.functions import col\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pyspark.sql.types import *\n",
    "import datetime\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "\n",
    "### Reading in the Data\n",
    "taxi_type = \"yellow\"\n",
    "data_dir = 'data/' + taxi_type + '*'\n",
    "sdf = spark.read.csv(data_dir, header=True)\n",
    "f\"{sdf.count():,} rows!\"\n",
    "\n",
    "def create_schema(data_dir):\n",
    "    ints = ['VendorID', 'passenger_count', 'RateCodeID', 'RatecodeID','payment_type', 'PULocationID', 'DOLocationID']\n",
    "    doubles = ['trip_distance','fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge']\n",
    "    strings = ['store_and_fwd_flag']\n",
    "    dtimes = ['tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    "    \n",
    "    if taxi_type == \"green\":\n",
    "        doubles.append(\"ehail_fee\")\n",
    "        ints.append(\"trip_type\")\n",
    "        dtimes = ['lpep_pickup_datetime', 'lpep_dropoff_datetime']\n",
    "\n",
    "    dtypes = {column: IntegerType() for column in ints}\n",
    "    dtypes.update({column: DoubleType() for column in doubles})\n",
    "    dtypes.update({column: StringType() for column in strings})\n",
    "    dtypes.update({column: TimestampType() for column in dtimes})\n",
    "\n",
    "    schema = StructType()\n",
    "\n",
    "    for column in sdf.columns:\n",
    "        schema.add(column, # column name\n",
    "                   dtypes[column], # data type\n",
    "                   True # is nullable?\n",
    "                  )\n",
    "    return schema\n",
    "\n",
    "schema = create_schema(data_dir)\n",
    "sdf = spark.read.csv(data_dir, header=True, schema=schema) \\\n",
    "    .withColumnRenamed(\"RatecodeID\",\"RateCodeID\") # rename the wrong column\n",
    "# Renaming lpep columns to tpep, for consistency so that it worths with later cuntions\n",
    "sdf = sdf.withColumnRenamed(\"lpep_pickup_datetime\",\"tpep_pickup_datetime\")\n",
    "sdf = sdf.withColumnRenamed(\"lpep_dropoff_datetime\",\"tpep_dropoff_datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6231fa8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:======================================================>  (18 + 1) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first entry for a cab is at 2020-01-01 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Data cleaning\n",
    "# Removing entries not actually in 2020\n",
    "sdf = sdf.filter((sdf.tpep_pickup_datetime >= datetime.datetime(2020,1,1,0,0,0)) & \\\n",
    "    (sdf.tpep_pickup_datetime < datetime.datetime(2021,1,1,0,0,0)))  \n",
    "# Finding the min time, to verify it is infact the first day of 2020\n",
    "time = sdf.select(pyspark.sql.functions.min(\"tpep_pickup_datetime\")).collect()[0][0]\n",
    "print(f\"The first entry for a cab is at {time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b73a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting the proportions of cars that are hailed, vs ordered, vs NA\n",
    "# Only applicable to green cab\n",
    "nums = list(sdf.groupby(\"trip_type\").count().collect()) # Getting counts of trip types\n",
    "nums = list(map(lambda x: list(x)[1], nums))    # Flattening\n",
    "nums = list(map(lambda x: x/sum(nums), nums))   # Converting to percentage of total\n",
    "print(f\"NULL: {nums[0]*100:.2f}%, Street Hail: {nums[1]*100:.2f}%, Dispath: {nums[2]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6705b91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the average number of pickups in a 10 minute interval in a given PU location ID\n",
    "print(f\"There are {sdf.count()} trips in total and {sdf.select('PULocationID').distinct().count()} unique PULocationIDs.\")\n",
    "num = sdf.count()/sdf.select(\"PULocationID\").distinct().count()/365/24/6\n",
    "print(f\"In a 10 minute interval in a given location ID there are on average {num:.2f} trips.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb5795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the number of location IDs\n",
    "num_locations = max([sdf.select(pyspark.sql.functions.max(\"PULocationID\")).collect()[0][0], sdf.select(pyspark.sql.functions.max(\"DOLocationID\")).collect()[0][0]])\n",
    "temp_sdf_date = datetime.datetime(2020,1,1)\n",
    "curr_pickup_time = datetime.datetime(2020,1,1,0,0,0)\n",
    "\n",
    "def get_pickups(time_interval_in_seconds):\n",
    "    global curr_pickup_time\n",
    "    num_pickups = list([0]*num_locations)\n",
    "    \n",
    "    sdf_within_interval = temp_sdf.filter((temp_sdf.tpep_pickup_datetime >= curr_pickup_time) & \\\n",
    "        (temp_sdf.tpep_pickup_datetime < \\\n",
    "        curr_pickup_time + datetime.timedelta(seconds = time_interval_in_seconds)))\n",
    "    counts = sdf_within_interval.groupby(\"PULocationID\").count().collect()\n",
    "    for count in counts:\n",
    "        num_pickups[count[0]-1] = count[1]\n",
    "    \n",
    "    curr_pickup_time += datetime.timedelta(seconds = time_interval_in_seconds)\n",
    "    if curr_pickup_time > temp_sdf_date:\n",
    "        populate_temp_sdf()\n",
    "    return num_pickups\n",
    "\n",
    "# Creates a df with the next days events, returns a bool denoting success\n",
    "def populate_temp_sdf():\n",
    "    global temp_sdf_date\n",
    "    \n",
    "    if temp_sdf_date == datetime.datetime(2021, 1, 1):\n",
    "        return None\n",
    "    \n",
    "    temp_sdf = sdf.filter((sdf.tpep_pickup_datetime >= temp_sdf_date) & \\\n",
    "        (sdf.tpep_pickup_datetime < temp_sdf_date + datetime.timedelta(days=1)))\n",
    "    temp_sdf.cache()\n",
    "    \n",
    "    temp_sdf_date += datetime.timedelta(days=1)\n",
    "    \n",
    "    return temp_sdf\n",
    "\n",
    "temp_sdf = populate_temp_sdf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "fig = plt.figure()\n",
    "#creating a subplot \n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "def animate(i):\n",
    "    ax1.clear()\n",
    "    \n",
    "    plt.title(f'Pick Up Density over Time - {curr_pickup_time}')\n",
    "    plt.xlabel('Location IDs')\n",
    "    plt.ylabel('Number of Pickups')\n",
    "    plt.ylim(top=100)\n",
    "    \n",
    "    locations = list(range(1, num_locations+1))\n",
    "    num_pickups = get_pickups(60*10)\n",
    "    ax1.bar(locations, num_pickups)\n",
    "    \n",
    "ani = animation.FuncAnimation(fig, animate, interval=250, frames=6*23) \n",
    "ani.save('sine_wave.gif', writer='imagemagick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2ec520",
   "metadata": {},
   "outputs": [],
   "source": [
    "### A small test of regression in spark\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "tiny_sdf = spark.read.csv(\"tmp.csv\", header = True)\n",
    "schema = StructType()\n",
    "ints = ('MATH', 'CS', 'Score')\n",
    "dtypes = {column: IntegerType() for column in ints}\n",
    "schema = StructType()\n",
    "\n",
    "for column in tiny_sdf.columns:\n",
    "    schema.add(column, # column name\n",
    "               dtypes[column], # data type\n",
    "               True # is nullable?\n",
    "              )\n",
    "\n",
    "tiny_sdf = spark.read.csv(\"tmp.csv\", header = True, schema=schema)\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['MATH', 'CS'], outputCol = 'features')\n",
    "tiny_sdf = vectorAssembler.transform(tiny_sdf)\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, labelCol='Score', featuresCol = 'features')\n",
    "model = lr.fit(tiny_sdf)\n",
    "tiny_sdf.show()\n",
    "for i in range(4):\n",
    "    model.predict(tiny_sdf.head(4)[i].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "918ff9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+\n",
      "|day|time_interval|PULocationID|\n",
      "+---+-------------+------------+\n",
      "|  1|           21|         138|\n",
      "|  1|          123|         140|\n",
      "|  1|          101|         132|\n",
      "|  1|            6|         238|\n",
      "|  1|          113|         238|\n",
      "+---+-------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Reducing sdf into a structure containing average pickups for each location for each time for each time interval\n",
    "def transform(x):\n",
    "    time = x.tpep_pickup_datetime\n",
    "    time_interval = 20\n",
    "    time_interval_index = ((time.hour*3600) + (time.minute*60) + (time.second))//time_interval\n",
    "    return time.weekday(), time_interval_index, x.PULocationID\n",
    "\n",
    "from os import path\n",
    "if path.isdir(f\"Intervaled_{taxi_type}\"):\n",
    "    ### Reading an existing sdf2\n",
    "    sdf2 = spark.read.csv(f\"Intervaled_{taxi_type}\", header=True)\n",
    "    ints = [\"day\", \"time_interval\", \"PULocationID\"]\n",
    "    dtypes = {column: IntegerType() for column in ints}\n",
    "    schema = StructType()\n",
    "    [schema.add(column, dtypes[column], True) for column in sdf2.columns]\n",
    "    sdf2 = spark.read.csv(f\"Intervaled_{taxi_type}\", header=True, schema = schema)\n",
    "else:\n",
    "    rdd2 = sdf.rdd.map(transform)\n",
    "    sdf2 = rdd2.toDF([\"day\",\"time_interval\",\"PULocationID\"])\n",
    "    sdf2.write.csv(f\"Intervaled_{taxi_type}\", header = True)\n",
    "    sdf2.persist(pyspark.StorageLevel.DISK_ONLY)\n",
    "\n",
    "sdf2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc4bcef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-----+\n",
      "|day|time_interval|PULocationID|count|\n",
      "+---+-------------+------------+-----+\n",
      "|  3|         2728|         236|  129|\n",
      "|  2|         2674|         237|  127|\n",
      "|  3|         2670|         237|  123|\n",
      "|  1|         2687|         237|  123|\n",
      "+---+-------------+------------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[day: int, time_interval: int, PULocationID: int, count: bigint]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create a smaller version of sdf2 and try the aggregations\n",
    "sdf3 = sdf2.groupby([\"day\", \"time_interval\", \"PULocationID\"]).count().sort(col(\"count\").desc())\n",
    "sdf3.show(4)\n",
    "sdf3.persist(pyspark.StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13e57322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-----+-------------+-------------------+-----------------+\n",
      "|day|time_interval|PULocationID|count|      day_vec|  time_interval_vec| PULocationID_vec|\n",
      "+---+-------------+------------+-----+-------------+-------------------+-----------------+\n",
      "|  3|         2728|         236|  129|(6,[3],[1.0])|(4319,[2728],[1.0])|(265,[236],[1.0])|\n",
      "|  2|         2674|         237|  127|(6,[2],[1.0])|(4319,[2674],[1.0])|(265,[237],[1.0])|\n",
      "|  3|         2670|         237|  123|(6,[3],[1.0])|(4319,[2670],[1.0])|(265,[237],[1.0])|\n",
      "|  1|         2687|         237|  123|(6,[1],[1.0])|(4319,[2687],[1.0])|(265,[237],[1.0])|\n",
      "|  2|         2756|         237|  122|(6,[2],[1.0])|(4319,[2756],[1.0])|(265,[237],[1.0])|\n",
      "+---+-------------+------------+-----+-------------+-------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "sdf4 = OneHotEncoder(inputCols = [\"day\",\"time_interval\",\"PULocationID\"], outputCols = [\"day_vec\",\"time_interval_vec\",\"PULocationID_vec\"]).fit(sdf3).transform(sdf3)\n",
    "sdf4.show(5)\n",
    "vectorAssembler = VectorAssembler(inputCols = [\"day_vec\",\"time_interval_vec\",\"PULocationID_vec\"], outputCol = 'features')\n",
    "sdf4 = vectorAssembler.transform(sdf4)\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, labelCol='count', featuresCol = 'features')\n",
    "\n",
    "splits = sdf4.randomSplit([0.7, 0.3])\n",
    "train_sdf4 = splits[0]\n",
    "test_sdf4 = splits[1]\n",
    "                 \n",
    "lr_model = lr.fit(train_sdf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f61a7f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 9.895082\n",
      "r2: 0.483622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+--------------------+\n",
      "|       prediction|count|            features|\n",
      "+-----------------+-----+--------------------+\n",
      "|26.23592760189126|   79|(4590,[0,1189,451...|\n",
      "|26.23592760189126|   74|(4590,[0,1211,451...|\n",
      "|26.23592760189126|   74|(4590,[0,1218,451...|\n",
      "|26.23592760189126|   77|(4590,[0,1222,451...|\n",
      "|26.23592760189126|   77|(4590,[0,1224,451...|\n",
      "+-----------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 431:=====================================================> (47 + 1) / 48]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on test data = 0.483943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trainingSummary = model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)\n",
    "lr_model.evaluate(train_sdf4)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_predictions = lr_model.transform(test_sdf4)\n",
    "lr_predictions.select(\"prediction\",\"count\",\"features\").show(5)\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"count\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0877f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
